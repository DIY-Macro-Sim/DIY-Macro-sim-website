# An Introduction to Stability Analysis of Dynamic Models

## Solution of a single first-order linear difference equation

Consider a first-order linear difference equation:[^how_to_simulate-1]

[^how_to_simulate-1]: We will focus here on difference instead of differential equations, i.e. on dynamics in discrete as opposed to continuous time. Most of the continuous-time counterpart is analogous to the material covered here. @Sayama2015 provides a very accessible and applied introduction to dynamic systems with Python code. An introductory treatment of the underlying mathematics is @Chiang2005, chaps. X. @Gandolfo2009 provides a more advanced treatment of the mathematics as well as many economic examples. A great introduction to linear algebra is @Anthony2012.

$$
y_t = a_0 + a_1y_{t-1}.
$$

One way to find a solution is through (manual) iteration:

$y_0$, 
$y_1 = a_0 + a_1 y_0$, 
$y_2 = a_0 + a_1(a_0 + a_1 y_0) = a_0^2y_0 + a_0(a_1+1)$,
..., 
$y_t = a_1^ty_0 + a_0\sum^{t-1}_{i=0} a_1^t$. 

Of course, this is the same approach we have used above to solve economic models via simulation. If $|a_1|<1$, we can make use of the fact that the second term is a geometric series which approaches $\frac{a_0}{1-a}$ as $t\rightarrow \infty$. Another way of finding this second term in the solution is to set $y_t=y_{t-1}$ and solving for $y$. Thus, the solution takes the form: 

$$
y_t = a_1^ty_0 + \frac{a_0}{1-a_1}.
$$
From iteration, we thus know that the solution to a difference equation has two parts: first, a term that captures the dynamics of $y_t$ (the so-called *complementary function*) and second, a term that captures the long-run equilibrium $y^*$ (the so-called *particular solution*). The complementary function tells us about the 'asymptotic stability' of the equation: does $y_t$ converge to $y^*$ as $t \rightarrow \infty$? For the case of a first-order difference equation, we can distinguish the following cases:

-   if $|a_1|<1$, then the complementary function $a_1^ty_0$ will converge to zero and $y_t$ will approach the particular solution $y^*=\frac{a_0}{1-a_1}$

-   if $|a_1|>1$, then the complementary function $a_1^ty_0$ will grow exponentially or decay and $y_t$ will thus never converge to the particular solution

-   if $a_1=1$ and $a_0 \neq$, then $y_t$ will grow linearly

-   if $a_1=1$ and $a_0 =0$, then $y_t$ will not grow o fall forever, but it will also not approach the particular solution.

To better understand the last two cases, note that if $a_1=1$, a different (more general) approach to finding the particular solution is required: the so-called method of undetermined coefficients. This method consists of substituting a trial solution into the difference equation that contains undetermined coefficients and then attempting to solve for those coefficients. If the trial solution allows to pin down unique values for the coefficients, it constitute a valid particular solution. In the case where $a_1=1$ and $a_0 \neq 0$, use the trial solution $y^*=kt$, i.e. consider a growing equilibrium. This yields $y_t = k(t-1) + a_0$, which solves for $k=a_0$, so that we can conclude $y^*=a_0t$. This explains why we obtain linear growth.

## Solution of linear system of difference equations

The solution approach just introduced can be extended to linear systems of difference equations of the form:

$$
\mathbf{y_t}=\mathbf{a_0} + \mathbf{Ay_{t-1}}.  
$$

The solution will be of the form:

$$
\mathbf{y_t}=\mathbf{A^t}\mathbf{y_0} + (\mathbf{I}-\mathbf{A})^{-1}\mathbf{a_0}.
$$

The problem with this generic solution is that it is difficult to assess what is going on: the dynamics of any variable in $\mathbf{y_t}$ will depend on the initial values of the other variables and a lengthy combination of the parameters in $A$ that result from repeated matrix multiplication. In addition, it is impossible to assess whether the system converges to the particular solution. To address this problem, we can use a tool from linear algebra called matrix diagonalisation. Under certain conditions, a matrix $A$ can be decomposed into the product of three matrices in which the matrix in the middle is diagonal. As we will see, this trick has a useful application for our problem.

A matrix $A$ is diagonalisable if there is a diagonal matrix $D$ and an invertible matrix $P$ such that $A=PDP^{-1}$. A major advantage of this decomposition is the following property: $A^n = (PDP^{-1})^n = PD^nP^{-1}$.[^how_to_simulate-3] Thus, the $n-th$ power of the matrix $A$, which typically yields very cumbersome expressions, simplifies to $PD^nP^{-1}$, where the $n-th$ power of $D$ are simply applied to each individual element on the main diagonal thanks to $D$ being a diagonal matrix. As a result, diagonalisation allows us to write the complementary function in the solution to a system of difference equations as: $PD^tP^{-1}y_0$. We can further define a vector of arbitrary constants $c=P^{-1}y_0$ so that the complementary function becomes $PD^tc$. For the first variable in the system, the solution would be of the form: $y_{1t}=v_11c_1\lambda_1^t+v_12c_2\lambda_2^t + ...+ y^*$. From this representation of the solution, the nature of the dynamics can easily be determined by looking at the eigenvalue $\lambda$ that is largest in absolute terms or 'dominant'. Only if the dominant eigenvalue is $|\lambda|<1$ will the system converge to $y^*$. The elements $v_{ij}$ of the eigenvectors act as multipliers on the eigenvalues and can thus switch off certain eigenvalues of they happen to be zero or amplify their dynamics both into the positive and negative domain, depending on their algebraic sign.

[^how_to_simulate-3]: This is because in the product $(PDP^{-1})(PDP^{-1})(PDP^{-1})...$, each $P$ cancels a $P^{-1}$, except for the first $P$ and last $P^{-1}$.

How can the diagonal matrix $D$ be found? Notice that $AP=PD$ can also be written as $Av_i=\lambda_iv_i$, where $v_i$ are the column vectors of $P$ and $\lambda_i$ are the elements on the main diagonal of $D$. The $v_i$ are called the eigenvectors of the matrix $A$ and the $\lambda_i$ are its eigenvalues.Wen can then write $v_i(A-\lambda_iI)=0$. We want to find the solutions of this linear system other than $v_i \neq 0$. This requires the determinant of the matrix $A-\lambda_iI$ to become zero, i.e. $det(A-\lambda_iI)=0$. Note there will be an infinite number of solutions for the eigenvectors. In fact, any scalar multiple of the eigenvectors (other than zero) is admissible. It is thus common to normalise the eigenvectors by dividing through one of its elements.

Consider the matrix $A=\begin{bmatrix}7 & -15 \\ 2 & -4 \end{bmatrix}$. For simplicity, let $\mathbf{a_0}=\mathbf{0}$. Then $A-\lambda_iI =\begin{bmatrix}7 - \lambda & -15 \\ 2 & -4 - \lambda \end{bmatrix}$ and $det(A-\lambda_iI)=(7-\lambda)(-4-\lambda)+30=\lambda^2 - 3\lambda +2=0$. This second-order polynomial solves for $\lambda_1=2$ and $\lambda_2=1$, which will be the elements on the diagonal of $D$. To find $v_i$, substitute $\lambda_i$ into $A-\lambda_iI=0$. For $\lambda_1=2$, we get $5v_{21}- 15v_{22}=0$ and $2v_{21}- 6v_{22}=0$, yielding the eigenvector $v_1=\begin{bmatrix} 3 \\ 1\end{bmatrix}$. Dividing through by the first element yields the normalised eigenvector $v_1=\begin{bmatrix} 1 \\ \frac{1}{3} \end{bmatrix}$. For $\lambda_2=1$, this yields $6v_{11}- 15v_{12}=0$ and $2v_{11}-5v_{12}=0$ from which we can deduce that $v_2=\begin{bmatrix} 5 \\ 2\end{bmatrix}$. The normalised eigenvector is $v_2=\begin{bmatrix} 1 \\ 0.4 \end{bmatrix}$.

Of course, you can also perform these calculations in R:

```{r how_to9}
#Clear the environment 
rm(list=ls(all=TRUE))

## Find eigenvalues and eigenvectors of matrix
# Define matrix
J=matrix(c(7, -15,
           2, -4), 2, 2, byrow=TRUE)

# Obtain eigenvalues and eigenvectors
ev=eigen(J)
(evals = ev$values)
(evecs = ev$vector)

# Normalise eigenvectors by dividing through by the first element
evecs_norm=evecs
for (i in 1:2){
  evecs_norm[,i]=evecs[,i]/evecs[1,i]
}
evecs_norm
```

We can now use this solution for the eigenvectors and eigenvalues to write the solution of the dynamic system as:

$$
\begin{bmatrix} y_{1t} \\ y_{2t} \end{bmatrix}=\begin{bmatrix} 1 & 1 \\ \frac{1}{3} & 0.4 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ 0 & 1 \end{bmatrix}^t \begin{bmatrix} c_{1} \\ c_{2} \end{bmatrix}.
$$ Multiplying the matrices out yields:

$$
 y_{1t} = c_{1}2^t + c_{2}1^t \\
 y_{2t} = \frac{1}{3}c_{1}2^t + 0.4c_{2}1^t 
$$

Before comparing these analytical results with those from a numerical simulation, let's summarise the information we gain from the eigenvalues, eigenvectors, and arbitrary constants about the dynamics of the system:

-   since the dominant eigenvalue $\lambda_1=2$ is larger than one, we know that the system is unstable

-   since both elements in the dominant eigenvector $v_1=\begin{bmatrix} 1 \\ \frac{1}{3} \end{bmatrix}$ are non-zero, both variable in the system will be driven by that dominant eigenvalue

-   since both elements in the dominant eigenvector are positive but the arbitrary constant $c_1$ is negative, both variables will exhibit negative growth

-   since both variables will decay at the same rate, their ratio will be constant as $t \rightarrow \infty$ and will approach a value that is given by the ratio of the elements in the dominant eigenvector

To see the last point, observe that in $\frac{y_{2t}}{y_{1t}}=\frac{\frac{1}{3}c_{1}2^t + 0.4c_{2}1^t}{c_{1}2^t + c_{2}1^t}$, the terms first term in the numerator and denominator quickly dominates the second term as $t \rightarrow \infty$. Thus, $\frac{y_{2t}}{y_{1t}}$ will approach $\frac{1}{3}$ as $t \rightarrow \infty$.

Let us simulate the system and compare the results for, say, $t=20$ with the analytical solution:

```{r how_to10}

library(matlib)

# Set number of periods for which you want to simulate
T=100

# Construct matrices in which values for different periods will be stored; initialise at 1
y1=matrix(data=1, nrow=1, ncol=T)
y2=matrix(data=1, nrow=1, ncol=T)

#Solve this system recursively based on the initialisation
  for (t in 2:T){
    y1[,t] = J[1,1]*y1[, t-1] + J[1,2]*y2[, t-1]
    y2[,t] = J[2,1]*y1[, t-1] + J[2,2]*y2[, t-1]
} # close time loop


# Plot dynamics of y1
plot(y1[1, 1:15],type="l", col=1, lwd=2, lty=1, xlab="Time", ylab="y1") 
title(main="", cex=0.8)

# Find arbitrary constants: c=(P^-1)*y0
y0=c(y1[1,1],y2[1,1])  # create vector with initial conditions y0
c=inv(evecs_norm)%*%y0
c

# Compute solution manually for t=20 and compare with simulated solution
t=10
c[1,1]*evals[1]^t + c[2,1]*evals[2]^t
y1[,t+1]

evecs_norm[2,1]*c[1,1]*evals[1]^t + evecs_norm[2,1]*c[2,1]*evals[2]^t
y2[,t+1]

# Plot dynamics of y2/y1
y2_y1=y2/y1
plot(y2_y1[, 1:50],type="l", col=1, lwd=2, lty=1, xlab="Time", ylab="y2/y1")
title(main="", cex=0.8)

# Compare y2/y1 with normalised dominant eigenvector
y2_y1[,T]
evecs_norm[2,1]

```

It can be seen that the simulated results are equivalent to the results we obtain mathematically. The key insight is that by deriving information about the eigenvalues (and possibly eigenvectors) of the Jacobian matrix of the system, we are able to deduce knowledge of the dynamic properties of the system even without numerical simulation. However, the more complex the dynamic system, the more difficult this will be, thereby rendering numerical simulation a key tool to supplement formal analysis.

## Complex numbers and cycles

## Nonlinear systems
